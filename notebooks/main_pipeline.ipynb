{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875d40d3-ff3c-463d-af55-fc5c7e5faaf4",
   "metadata": {},
   "source": [
    "# 1. 导入 + 设置路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ff20d98-8fb0-48b7-8fe1-3c09a763b4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('unable to open database file')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 添加自定义模块路径\n",
    "sys.path.append(os.path.abspath(\"../utils\"))\n",
    "sys.path.append(os.path.abspath(\"../models\"))\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))\n",
    "sys.path.append(\"/root/Implementation\") \n",
    "sys.path.append(\"../dataset\")  # 如果你在 notebooks/ 中\n",
    "\n",
    "from train import train\n",
    "from simple_cnn import Simple3DCNNEncoder\n",
    "from decoder3D import LightDecoder\n",
    "from encoder3D import SparseEncoder\n",
    "from AnatoMask import SparK\n",
    "from real_dataset import Real3DMedicalDataset, SegmentationPatchDataset,RareClassPatchDataset\n",
    "from checkpoint import save_checkpoint, load_checkpoint\n",
    "from visualize import show_slice_comparison, show_slice_with_error\n",
    "from scripts.preprocess_segmentation_amos import preprocess_segmentation_amos\n",
    "from segmentation_dataset import SegmentationDataset\n",
    "from segmentation_model import SegmentationModel\n",
    "from losses import segmentation_loss\n",
    "from train_segmentation import train_full_segmentation\n",
    "from FullVolumeDataset import FullVolumeDataset\n",
    "\n",
    "\n",
    "from STUNet_head import STUNet  # ✅ 不是 STUNet.py 的\n",
    "from encoder3D import SparseEncoder\n",
    "from decoder3D import SMiMTwoDecoder  # 或 LightDecoder\n",
    "from AnatoMask import SparK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1c96f98-aafb-4ece-9226-4f1ccaa2c3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/root/miniconda/lib/python38.zip', '/root/miniconda/lib/python3.8', '/root/miniconda/lib/python3.8/lib-dynload', '', '/root/miniconda/lib/python3.8/site-packages', '/root/Implementation/utils', '/root/Implementation/models', '/root/Implementation/scripts', '/root/Implementation', '../dataset', '/root/Implementation/utils', '/root/Implementation/models', '/root/Implementation/scripts', '/root/Implementation', '../dataset', '/root/Implementation/utils', '/root/Implementation/models', '/root/Implementation/scripts', '/root/Implementation', '../dataset']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a02213-1089-4289-90ef-9484cf710083",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62a4e8d0-79ec-45c4-a2d6-167b62736aef",
   "metadata": {},
   "source": [
    "from scripts.preprocess_amos import preprocess_amos_nii_to_npy\n",
    "\n",
    "input_dir = \"/root/lanyun-tmp/amos_dataset/amos22/imagesTr\"\n",
    "output_dir = \"/root/lanyun-tmp/amos_dataset/amos22/npy_patches\"\n",
    "preprocess_amos_nii_to_npy(input_dir, output_dir, patch_size=(64, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed5822d-f0f5-458c-88d6-f6abee4b4b4a",
   "metadata": {},
   "source": [
    "# 2. 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec040721-d036-4920-93fe-bef959b28a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Config ======\n",
    "PATCH_SIZE = (64, 64, 64)\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 10\n",
    "LR = 1e-4\n",
    "DATA_DIR = \"/root/lanyun-tmp/amos_dataset/amos22/npy_patches\"\n",
    "CHECKPOINT_PATH = \"/root/lanyun-tmp/checkpoints/my_model.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37228098-02f2-4cf6-823e-b2f557b7f25d",
   "metadata": {},
   "source": [
    "#  3. 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "919d4a6f-7665-4bfb-bcdc-c796f4d2277d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparK.__init__, densify 1/5]: use nn.Identity() as densify_proj\n",
      "[SparK.__init__, densify 2/5]: densify_proj(ksz=3, #para=1.77M)\n",
      "[SparK.__init__, densify 3/5]: densify_proj(ksz=3, #para=0.44M)\n",
      "[SparK.__init__, densify 4/5]: densify_proj(ksz=3, #para=0.11M)\n",
      "[SparK.__init__, densify 5/5]: densify_proj(ksz=3, #para=0.03M)\n",
      "[SparK.__init__] dims of mask_tokens=(512, 256, 128, 64, 32)\n"
     ]
    }
   ],
   "source": [
    "# 定义输入 patch 尺寸\n",
    "PATCH_SIZE = (64, 64, 64)\n",
    "DIMS = [32, 64, 128, 256, 512, 512]\n",
    "POOL_KERNELS = [[2, 2, 2]] * 5\n",
    "CONV_KERNELS = [[3, 3, 3]] * 6\n",
    "\n",
    "# 构造 STU-Net\n",
    "cnn = STUNet(\n",
    "    input_channels=1,\n",
    "    num_classes=1,\n",
    "    depth=[1, 1, 1, 1, 1, 1],\n",
    "    dims=DIMS,\n",
    "    pool_op_kernel_sizes=POOL_KERNELS,\n",
    "    conv_kernel_sizes=CONV_KERNELS,\n",
    "    enable_deep_supervision=False  # 预训练时不需要\n",
    ")\n",
    "\n",
    "# 包装成 sparse encoder\n",
    "encoder = SparseEncoder(cnn, input_size=PATCH_SIZE, sbn=False)\n",
    "\n",
    "# 构造 decoder\n",
    "decoder = SMiMTwoDecoder(up_sample_ratio=encoder.downsample_ratio,\n",
    "                         width=encoder.enc_feat_map_chs[-1],  # = 512\n",
    "                         sbn=False)\n",
    "\n",
    "# 构造完整 SparK 模型\n",
    "model = SparK(\n",
    "    sparse_encoder=encoder,\n",
    "    dense_decoder=decoder,\n",
    "    mask_ratio=0.6  # SparK 预训练遮盖比例\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2387c42-cdec-4d34-9e1b-bbb2dc546d3c",
   "metadata": {},
   "source": [
    "# 4. 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbcb56f5-4eb5-426a-975a-8d6ce1ef74e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Real3DMedicalDataset(DATA_DIR)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eab6a4-18bd-4ed1-9f00-529bea7353f2",
   "metadata": {},
   "source": [
    "# 5. 启动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3e2df-d59b-43b3-ae0f-bd20fe35c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/root/miniconda/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 370, in reduce_storage\n",
      "    df = multiprocessing.reduction.DupFd(fd)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/reduction.py\", line 198, in DupFd\n",
      "    return resource_sharer.DupFd(fd)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/resource_sharer.py\", line 53, in __init__\n",
      "    self._id = _resource_sharer.register(send, close)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/resource_sharer.py\", line 77, in register\n",
      "    self._start()\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/resource_sharer.py\", line 130, in _start\n",
      "    self._listener = Listener(authkey=process.current_process().authkey)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/connection.py\", line 442, in __init__\n",
      "    address = address or arbitrary_address(family)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/connection.py\", line 76, in arbitrary_address\n",
      "    return tempfile.mktemp(prefix='listener-', dir=util.get_temp_dir())\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/util.py\", line 146, in get_temp_dir\n",
      "    tempdir = tempfile.mkdtemp(prefix='pymp-')\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/lib/python3.8/tempfile.py\", line 358, in mkdtemp\n",
      "    _os.mkdir(file, 0o700)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "OSError: [Errno 28] No space left on device: '/tmp/pymp-e45qwe84'\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/root/miniconda/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 370, in reduce_storage\n",
      "    df = multiprocessing.reduction.DupFd(fd)\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/reduction.py\", line 198, in DupFd\n",
      "    return resource_sharer.DupFd(fd)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/resource_sharer.py\", line 53, in __init__\n",
      "    self._id = _resource_sharer.register(send, close)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/resource_sharer.py\", line 77, in register\n",
      "    self._start()\n",
      "  File \"/root/miniconda/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 370, in reduce_storage\n",
      "    df = multiprocessing.reduction.DupFd(fd)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/resource_sharer.py\", line 130, in _start\n",
      "    self._listener = Listener(authkey=process.current_process().authkey)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/reduction.py\", line 198, in DupFd\n",
      "    return resource_sharer.DupFd(fd)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/connection.py\", line 442, in __init__\n",
      "    address = address or arbitrary_address(family)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/resource_sharer.py\", line 53, in __init__\n",
      "    self._id = _resource_sharer.register(send, close)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/connection.py\", line 76, in arbitrary_address\n",
      "    return tempfile.mktemp(prefix='listener-', dir=util.get_temp_dir())\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/resource_sharer.py\", line 77, in register\n",
      "    self._start()\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/util.py\", line 146, in get_temp_dir\n",
      "    tempdir = tempfile.mkdtemp(prefix='pymp-')\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/resource_sharer.py\", line 130, in _start\n",
      "    self._listener = Listener(authkey=process.current_process().authkey)\n",
      "  File \"/root/miniconda/lib/python3.8/tempfile.py\", line 358, in mkdtemp\n",
      "    _os.mkdir(file, 0o700)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/connection.py\", line 442, in __init__\n",
      "    address = address or arbitrary_address(family)\n",
      "OSError: [Errno 28] No space left on device: '/tmp/pymp-ik2samy_'\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/connection.py\", line 76, in arbitrary_address\n",
      "    return tempfile.mktemp(prefix='listener-', dir=util.get_temp_dir())\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/util.py\", line 146, in get_temp_dir\n",
      "    tempdir = tempfile.mkdtemp(prefix='pymp-')\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/lib/python3.8/tempfile.py\", line 358, in mkdtemp\n",
      "    _os.mkdir(file, 0o700)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "OSError: [Errno 28] No space left on device: '/tmp/pymp-u2w09fvz'\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/root/miniconda/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 370, in reduce_storage\n",
      "    df = multiprocessing.reduction.DupFd(fd)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/reduction.py\", line 198, in DupFd\n",
      "    return resource_sharer.DupFd(fd)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/resource_sharer.py\", line 53, in __init__\n",
      "    self._id = _resource_sharer.register(send, close)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/resource_sharer.py\", line 77, in register\n",
      "    self._start()\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/resource_sharer.py\", line 130, in _start\n",
      "    self._listener = Listener(authkey=process.current_process().authkey)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/connection.py\", line 442, in __init__\n",
      "    address = address or arbitrary_address(family)\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/connection.py\", line 76, in arbitrary_address\n",
      "    return tempfile.mktemp(prefix='listener-', dir=util.get_temp_dir())\n",
      "  File \"/root/miniconda/lib/python3.8/multiprocessing/util.py\", line 146, in get_temp_dir\n",
      "    tempdir = tempfile.mkdtemp(prefix='pymp-')\n",
      "  File \"/root/miniconda/lib/python3.8/tempfile.py\", line 358, in mkdtemp\n",
      "    _os.mkdir(file, 0o700)\n",
      "OSError: [Errno 28] No space left on device: '/tmp/pymp-uh914ww0'\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, DEVICE, epochs=EPOCHS, dataloader=loader, save_path=CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9e17b5-0597-4a3a-82b2-d4a0d9bc194f",
   "metadata": {},
   "source": [
    "# 6. 加载训练好的模型（如已训练过）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd48afd-c168-4c14-ab0f-3c1c2041e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果你想加载已有的模型，设置这个变量为 True\n",
    "LOAD_PRETRAINED = True\n",
    "\n",
    "if LOAD_PRETRAINED:\n",
    "    model, optimizer, start_epoch = load_checkpoint(model, optimizer, CHECKPOINT_PATH, DEVICE)\n",
    "    print(f\"✅ 成功加载模型！从 epoch {start_epoch+1} 继续\")\n",
    "else:\n",
    "    print(\"⚠️ 未加载任何 checkpoint，重新开始训练\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7061e9-6930-488f-a838-83f26755cc9c",
   "metadata": {},
   "source": [
    "# 7. 可视化重建效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b31fbd-60db-45ca-a682-3634f3496142",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sample = dataset[0].unsqueeze(0).to(DEVICE)\n",
    "mask = model.mask(sample.shape[0], sample.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    original, masked, reconstructed = model(sample, active_b1ff=mask, vis=True)\n",
    "\n",
    "show_slice_with_error(original, masked, reconstructed, axis=2)\n",
    "# show_slice_comparison(original, masked, reconstructed, axis=2) 如果不需要重建误差图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd0d25-4ed5-4ee3-ae96-1de053d39543",
   "metadata": {},
   "source": [
    "# 8. 分割微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9a408e-2617-4e94-9083-f9b91d5e10bc",
   "metadata": {},
   "source": [
    "## 8.1 数据整理"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38973926-fe94-48d9-a1f1-c0afb382544b",
   "metadata": {},
   "source": [
    "img_dir = \"/root/lanyun-tmp/amos_dataset/amos22/imagesTr\"\n",
    "lbl_dir = \"/root/lanyun-tmp/amos_dataset/amos22/labelsTr\"\n",
    "out_img = \"/root/lanyun-tmp/amos_dataset/segmentation_npy/images\"\n",
    "out_lbl = \"/root/lanyun-tmp/amos_dataset/segmentation_npy/labels\"\n",
    "\n",
    "preprocess_segmentation_amos(img_dir, lbl_dir, out_img, out_lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9028fbc2-9b8c-44e7-a009-d503f5e1d080",
   "metadata": {},
   "source": [
    "## 8.2 构建 segmentation Dataset + DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836de50b-df0b-4715-97b0-d3232daec791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 确保你之前已经定义了 FullVolumeDataset 类\n",
    "\n",
    "# 替换为你的实际路径\n",
    "image_dir = \"/root/lanyun-tmp/amos_dataset/amos22/imagesTr\"\n",
    "label_dir = \"/root/lanyun-tmp/amos_dataset/amos22/labelsTr\"\n",
    "\n",
    "# 创建 Dataset 实例\n",
    "dataset = FullVolumeDataset(image_dir=image_dir, label_dir=label_dir)\n",
    "\n",
    "# 用 DataLoader 测试读取\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 取出一条数据\n",
    "image, label = next(iter(loader))\n",
    "\n",
    "print(f\"Image shape: {image.shape}\")   # [B, C, H, W, D]\n",
    "print(f\"Label shape: {label.shape}\")   # [B, H, W, D]\n",
    "\n",
    "# 可视化中间切片（中间的 axial 切片）\n",
    "mid_slice = image[0, 0, :, :, image.shape[-1] // 2].numpy()\n",
    "plt.imshow(mid_slice, cmap='gray')\n",
    "plt.title(\"Middle slice of image volume\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd0b924-95ef-4c78-9106-11804b76135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 提取整个标签 volume，维度: [1, H, W, D]\n",
    "label_volume = label[0].numpy()\n",
    "\n",
    "unique_classes = np.unique(label_volume)\n",
    "print(f\"Unique class IDs in this label: {unique_classes}\")\n",
    "print(f\"Total classes: {len(unique_classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb54832-aedd-4054-8671-b35cdc00265d",
   "metadata": {},
   "source": [
    "## 8.3 构建 Segmentation 微调模型（预训练 encoder + 分割 head）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49f087-6993-47cd-b8dd-cc0bcf37fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = SegmentationModel(\n",
    "    input_size=(768, 768, 90),\n",
    "    num_classes=16,\n",
    "    checkpoint_path=\"/root/lanyun-tmp/checkpoints/anatomask_real.pth\"\n",
    ").cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03480e84-a927-4ca6-a002-0c21a39b34c5",
   "metadata": {},
   "source": [
    "## 8.4 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d095b828-96d8-418a-8f16-95d088667ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a75a6-c276-4446-b4ba-deba93765e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataset = RareClassPatchDataset(\n",
    "    image_dir=\"/root/lanyun-tmp/amos_dataset/segmentation_npy/images\",\n",
    "    label_dir=\"/root/lanyun-tmp/amos_dataset/segmentation_npy/labels\",\n",
    "    focus_foreground=True  # ✅ 启用“只取含前景”的采样方式\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "train_full_segmentation(\n",
    "    model, loader, optimizer, device=\"cuda\",\n",
    "    epochs=10,\n",
    "    loss_fn=\"dice_ce\",\n",
    "    save_path=\"/root/lanyun-tmp/checkpoints/segmentation_finetune.pth\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df3fc36-a636-4d7d-9e38-3ac4689395d7",
   "metadata": {},
   "source": [
    "## 8.5 Sliding Window 全图预测 + 拼接还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca21b4-b876-4a05-8c38-746a2360c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_full_volume(model, volume_tensor, patch_size=(128, 128, 64), stride=(64, 64, 32), num_classes=16):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: segmentation model\n",
    "        volume_tensor: [1, D, H, W] torch.Tensor (single volume)\n",
    "        patch_size: spatial window\n",
    "        stride: spatial step for sliding\n",
    "        num_classes: number of segmentation classes\n",
    "\n",
    "    Returns:\n",
    "        full_pred: [num_classes, D, H, W] torch.Tensor\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    volume_tensor = volume_tensor.unsqueeze(0).to(device)  # → [1, 1, D, H, W]\n",
    "\n",
    "    C, D, H, W = volume_tensor.shape[1:]\n",
    "    output_volume = torch.zeros((1, num_classes, D, H, W), device=device)\n",
    "    count_map = torch.zeros((1, 1, D, H, W), device=device)\n",
    "\n",
    "    for z in range(0, D - patch_size[0] + 1, stride[0]):\n",
    "        for y in range(0, H - patch_size[1] + 1, stride[1]):\n",
    "            for x in range(0, W - patch_size[2] + 1, stride[2]):\n",
    "                patch = volume_tensor[:, :, z:z+patch_size[0], y:y+patch_size[1], x:x+patch_size[2]]\n",
    "                with torch.no_grad():\n",
    "                    pred = model(patch)  # [1, C, z, y, x]\n",
    "                    output_volume[:, :, z:z+patch_size[0], y:y+patch_size[1], x:x+patch_size[2]] += pred\n",
    "                    count_map[:, :, z:z+patch_size[0], y:y+patch_size[1], x:x+patch_size[2]] += 1\n",
    "\n",
    "    # Avoid division by zero\n",
    "    count_map = torch.clamp(count_map, min=1.0)\n",
    "    output_volume = output_volume / count_map\n",
    "\n",
    "    return output_volume.squeeze(0).softmax(dim=0)  # [C, D, H, W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2688f276-50a7-4370-9dd9-f83c4578f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "nii_path = \"/root/lanyun-tmp/amos_dataset/amos22/imagesTr/amos_0001.nii.gz\"\n",
    "nii_img = nib.load(nii_path)\n",
    "volume = torch.from_numpy(nii_img.get_fdata()).float().permute(2, 0, 1)  # [D, H, W]\n",
    "\n",
    "volume = (volume - volume.mean()) / (volume.std() + 1e-5)\n",
    "volume = volume.unsqueeze(0).unsqueeze(0)  # [1, 1, D, H, W]\n",
    "\n",
    "# Run prediction\n",
    "pred = predict_full_volume(model, volume.squeeze(0), patch_size=(128, 128, 64), stride=(64, 64, 32), num_classes=16)\n",
    "mask = pred.argmax(dim=0).cpu().numpy()  # [D, H, W]\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faa302a-eb45-4522-ac10-ea5e3c7ae1ba",
   "metadata": {},
   "source": [
    "## 8.6 结果保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162c7cc-a386-4315-950e-0b10a445f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import os\n",
    "\n",
    "def save_mask_as_nii(mask_np, reference_nii_path, output_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        mask_np: np.ndarray, shape: [D, H, W] (uint8 or int)\n",
    "        reference_nii_path: 用于对齐空间信息的原始图像路径\n",
    "        output_path: 保存路径\n",
    "    \"\"\"\n",
    "    ref_img = nib.load(reference_nii_path)\n",
    "    affine = ref_img.affine\n",
    "    header = ref_img.header\n",
    "\n",
    "    mask_img = nib.Nifti1Image(mask_np.astype(np.uint8), affine, header)\n",
    "    nib.save(mask_img, output_path)\n",
    "    print(f\"✅ Saved mask to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ae0cb1-530a-4c9d-919d-de2a81af1507",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mask_as_nii(\n",
    "    mask,  # ← 来自 predict_full_volume 的 argmax 结果\n",
    "    reference_nii_path=\"/root/lanyun-tmp/amos_dataset/amos22/imagesTr/amos_0001.nii.gz\",\n",
    "    output_path=\"/root/Implementation/predicted_masks/amos_0001_pred.nii.gz\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d6eaa-2fd4-4841-bf4a-15932de350e5",
   "metadata": {},
   "source": [
    "## 8.7 Dice per class 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c3c2b-63cd-452e-8366-4a2bfd4c3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dice_per_class(pred_mask, gt_mask, num_classes=16, ignore_background=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred_mask: np.ndarray, shape [D, H, W], predicted label map\n",
    "        gt_mask: np.ndarray, same shape, ground truth\n",
    "        num_classes: total number of classes (e.g. 16)\n",
    "        ignore_background: whether to exclude class 0\n",
    "\n",
    "    Returns:\n",
    "        dict: {class_idx: dice_score}\n",
    "    \"\"\"\n",
    "    dice_dict = {}\n",
    "    classes = range(1, num_classes) if ignore_background else range(num_classes)\n",
    "\n",
    "    for c in classes:\n",
    "        pred_c = (pred_mask == c).astype(np.uint8)\n",
    "        gt_c = (gt_mask == c).astype(np.uint8)\n",
    "\n",
    "        intersect = (pred_c * gt_c).sum()\n",
    "        denom = pred_c.sum() + gt_c.sum()\n",
    "\n",
    "        dice = (2. * intersect) / denom if denom > 0 else 1.0\n",
    "        dice_dict[c] = dice\n",
    "\n",
    "    return dice_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c43c8-08d0-4ebb-bf14-15ffebfed0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_path = \"/root/lanyun-tmp/amos_dataset/amos22/labelsTr/amos_0001.nii.gz\"\n",
    "gt = nib.load(gt_path).get_fdata().astype(np.uint8)\n",
    "gt = np.transpose(gt, (2, 0, 1))  # [D, H, W]\n",
    "\n",
    "dice_scores = compute_dice_per_class(mask, gt, num_classes=16)\n",
    "for c, d in dice_scores.items():\n",
    "    print(f\"Class {c}: Dice = {d:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36cedeb-f144-4932-80c6-06c08a683130",
   "metadata": {},
   "source": [
    "## 8.8 可视化分割结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d1a49-3e4b-4f54-9731-95e607a64ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_segmentation_slice(image_3d, mask_3d, slice_index=None, title=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image_3d: np.ndarray, shape [D, H, W]\n",
    "        mask_3d: np.ndarray, shape [D, H, W]\n",
    "        slice_index: which slice to visualize (default: center)\n",
    "        title: optional title for the plot\n",
    "    \"\"\"\n",
    "    D = image_3d.shape[0]\n",
    "    if slice_index is None:\n",
    "        slice_index = D // 2\n",
    "\n",
    "    img_slice = image_3d[slice_index]\n",
    "    mask_slice = mask_3d[slice_index]\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_slice, cmap='gray')\n",
    "    plt.title(f\"Original Slice {slice_index}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img_slice, cmap='gray')\n",
    "    plt.imshow(mask_slice, cmap='jet', alpha=0.5)  # ⬅️ overlay 分割 mask\n",
    "    plt.title(title or \"Overlay Segmentation\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da49220-3a0b-429c-b618-22ef3883b936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原图 [D, H, W]\n",
    "image = nib.load(\"/root/lanyun-tmp/amos_dataset/amos22/imagesTr/amos_0001.nii.gz\").get_fdata()\n",
    "image = np.transpose(image, (2, 0, 1))  # → [D, H, W]\n",
    "\n",
    "# 可视化中间切片\n",
    "visualize_segmentation_slice(image_3d=image, mask_3d=mask, slice_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb8b29c-9ce7-41e8-a85f-12c055e6de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_dir = \"/root/lanyun-tmp/amos_dataset/segmentation_npy/labels\"\n",
    "cls_hist = np.zeros(16)\n",
    "\n",
    "for f in os.listdir(label_dir):\n",
    "    y = np.load(os.path.join(label_dir, f))\n",
    "    hist = np.bincount(y.flatten(), minlength=16)\n",
    "    cls_hist += hist\n",
    "\n",
    "plt.bar(range(16), cls_hist)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Pixel Count\")\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4c7470-0cd0-436d-91a9-e2b5e2f5feae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
